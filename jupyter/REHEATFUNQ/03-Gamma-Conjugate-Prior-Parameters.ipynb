{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parameter Estimation for the Gamma Conjugate Prior\n",
    "In this notebook, parameters for the gamma conjugate prior (GCP) are estimated using the\n",
    "*least surprise estimate*, that is, by minimizing the maximum Kullback-Leibler distance\n",
    "of the GCP to any of the regional distributions.\n",
    "\n",
    "\n",
    "## Configuration\n",
    "First, the configuration of the fitting algorithm. As described in the REHEATFUNQ model\n",
    "description paper, the algorithm proceeds by randomly distributing disks across a spherical\n",
    "Earth, and for each disk selecting data from the filtered global heat flow database that\n",
    "are within the disk. The following parameters control the disk size and the disk acceptance\n",
    "criterion:\n",
    "\n",
    "| Parameter    | Purpose                                                                                       |\n",
    "| :---------   | :-------------------------------------------------------------------------------------------- |\n",
    "| `R`          | Radius of the disks, in meters.                                                               |\n",
    "| `DMIN_KM`    | Minimum distance between two data points in km. Select one of each violating pair at random.  |\n",
    "| `min_points` | Minimum number of selected points. Reject proposed disks if less selected points within.      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 80e3\n",
    "DMIN_KM = 20\n",
    "min_points = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "import numpy as np\n",
    "from pyproj import Proj\n",
    "from pathlib import Path\n",
    "from plotconfig import *\n",
    "from cmcrameri.cm import *\n",
    "from pdtoolbox.mle import *\n",
    "from cache import cached_call\n",
    "import matplotlib.pyplot as plt\n",
    "from pdtoolbox.likelihood import *\n",
    "from matplotlib.lines import Line2D\n",
    "from pickle import Pickler, Unpickler\n",
    "from pdtoolbox.distributions import *\n",
    "from scipy.optimize import root_scalar\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.patches import Rectangle\n",
    "from reheatfunq import GammaConjugatePrior\n",
    "from zeal2022hf import kindlmann, kindlmann_r\n",
    "from loaducerf3 import PolygonSelector, Polygon\n",
    "from reheatfunq.regional.backend import gamma_mle\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.patches import Polygon as MPolygon\n",
    "from reheatfunq.coverings import random_global_R_disk_coverings\n",
    "from shapely.geometry import Polygon as SPoly, MultiLineString as SMLS, LineString as SLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Load the data set processed in `01-Load-and-filter-NGHF.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_continental = np.load('intermediate/heat-flow-selection-mW_m2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude areas used in regional analyses\n",
    "Load the selectors that will later select the regional aggregate heat flow distributions.\n",
    "These polygon selectors have been defined in `02-Study-Area-Geometry.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate/02-Geometry.pickle','rb') as f:\n",
    "    saf_geometry = Unpickler(f).load()\n",
    "\n",
    "proj_saf = Proj(saf_geometry[\"proj_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(hf_continental.shape[1], dtype=bool)\n",
    "hf_xy = np.stack(proj_saf(*hf_continental[1:3,:]), axis=1)\n",
    "\n",
    "for poly in saf_geometry[\"selection_polygons_xy\"]:\n",
    "    select = PolygonSelector(Polygon(*poly[:-1].T))\n",
    "    mask &= ~select.array_mask(hf_xy)\n",
    "hf_independent = (hf_continental.T)[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-indexing to the NGHF data set\n",
    "Compute the indices of the masked data within the original NGHF data set (Lucazeau, 2019)\n",
    "for simple reproducibility. This might or might not be relevant for applications to other\n",
    "areas and/or data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_continental_indices = np.loadtxt('results/nghf-selection-indices.csv', delimiter=',', dtype=int)\n",
    "\n",
    "# We also have to update the indices into hf_continental now:\n",
    "local2continental = np.arange(mask.size)[mask]\n",
    "local2lucazeau = hf_continental_indices[local2continental]\n",
    "lucazeau2local = {i : j for i,j in enumerate(local2lucazeau)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a disk-buffer around the polygons\n",
    "In the later analysis based on the random distribution of disks all over\n",
    "Earth surface, we want to prevent the disks from overlapping with the selection\n",
    "polygons. Using the buffer functionality of Shapely, we can create buffered\n",
    "polygons (buffer by disk radius) that can prevent by rejecting any disk center\n",
    "whose center is within the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 80e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect('equal')\n",
    "buffered_shpoly = shapely.geometry.Polygon()\n",
    "for poly in saf_geometry[\"selection_polygons_xy\"]:\n",
    "    ax.plot(*poly.T, color='k')\n",
    "    shpoly = shapely.geometry.Polygon(poly).buffer(R)\n",
    "    buffered_shpoly = buffered_shpoly.union(shpoly)\n",
    "\n",
    "x,y = buffered_shpoly.exterior.coords.xy\n",
    "ax.plot(x,y,color='gray')\n",
    "buffered_poly = np.array(x[:-1]), np.array(y[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate/03-Buffered-Poly.pickle','wb') as f:\n",
    "    Pickler(f).dump(buffered_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Regional Distributions\n",
    "First off, prepare some code for spatial filtering. So far, we have not checked for double entries to the data base. Also, we might want to remove data points that are too close to each other (circumventing the spatial clustering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 498267187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_points, used_data_entries, distributions, distribution_lola, distribution_indices \\\n",
    "    = cached_call(random_global_R_disk_coverings, R, min_points,\n",
    "                  hf_independent, buffered_poly, saf_geometry[\"proj_str\"], dmin=DMIN_KM*1e3,\n",
    "                  MAX_DRAW=10000000, N=200, seed=seed)\n",
    "\n",
    "print(\"Number of distributions:  \",len(distributions))\n",
    "print(\"Maximum distribution size:\", max(d.size for d in distributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "# Histogram of sample size:\n",
    "ax = fig.add_subplot(131)\n",
    "ax.hist([d.size for d in distributions], bins=20)\n",
    "ax.set_xlabel('Regional sample size')\n",
    "ax.set_ylabel('Number of regions')\n",
    "\n",
    "# Global distribution:\n",
    "ax = fig.add_subplot(132)\n",
    "ax.scatter(*hf_independent[:,1:3].T, marker='.', edgecolor='none')\n",
    "ax.scatter(*np.array(valid_points).T, marker='x')\n",
    "ax.plot(*proj_saf(x, y, inverse=True), color='gray')\n",
    "\n",
    "# The study area:\n",
    "ax = fig.add_subplot(133)\n",
    "ax.plot(*proj_saf(x, y, inverse=True), color='gray')\n",
    "ax.set_xlim(ax.get_xlim())\n",
    "ax.set_ylim(ax.get_ylim())\n",
    "ax.scatter(*hf_independent[:,1:3].T, marker='.', edgecolor='none')\n",
    "ax.scatter(*np.array(valid_points).T, marker='x')\n",
    "\n",
    "\n",
    "Path('figures').mkdir(exist_ok=True)\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/03-RGRDC-sample-sizes-and-locations.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark the data entries that will be used in the later regional analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ude = []\n",
    "for i,d in enumerate(distributions):\n",
    "    ude.append([int(local2lucazeau[k])\n",
    "                for k in distribution_indices[i]])\n",
    "\n",
    "used_data_entries = ude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum-likelihood estimates to the regional distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.array([gamma_mle(dist, amin=1.0) for dist in distributions]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('results').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results/03-gamma-conjugate-prior-results.json','w') as f:\n",
    "    json.dump([(pts, entries, (a,b))\n",
    "               for pts, entries, a, b in zip(valid_points, used_data_entries, A, B)],\n",
    "              f)\n",
    "pts_feature_list = [{\"type\" : \"Feature\",\n",
    "                     \"geometry\" : {\n",
    "                         \"type\" : \"Point\",\n",
    "                         \"coordinates\" : pt,\n",
    "                     },\n",
    "                     \"properties\" : {\n",
    "                         \"a\" : a,\n",
    "                         \"b\" : b,\n",
    "                         \"lucazeau2019entries\" : entries\n",
    "                     }} for pt, entries, a, b in zip(valid_points, used_data_entries, A, B)]\n",
    "geojson_dict = { \"type\": \"FeatureCollection\",\n",
    "                 \"name\" : \"Ziebarth et al. (2021) gamma distribution fits to Lucazeau (2019) heat flow data\",\n",
    "                 \"license\" : \"CC-BY 4.0\",\n",
    "                 \"features\": pts_feature_list}\n",
    "with open('results/gamma-conjugate-prior-results.geojson','w') as f:\n",
    "    json.dump(geojson_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Gamma Conjugate Prior\n",
    "We compute the *minimum surprise estimate* (MSE) of the gamma conjugate prior given the distributions.\n",
    "The MSE minimizes the Kullback-Leibler (KL) distance between conjugate prior $\\phi(\\alpha,\\beta)$ and\n",
    "the heat flow data.\n",
    "In particular, for each aggregate heat flow distribution $Q_i = \\{q_j\\}_i$, a posterior probability\n",
    "$\\pi_i(\\alpha, \\beta)$ is computed from an \"uninformed\" prior ($p_i=1$, $s_i=n_i=v_i=0$, see Miller, 1980).\n",
    "Then, the maximum KL distance $d$ from the conjugate prior to any of the $\\pi_i$,\n",
    "$$\n",
    "    d = \\max\\limits_i \\left\\{ \\,\\int\\limits_{\\alpha_\\mathrm{min}}^\\infty\\!\\mathrm{d}\\alpha\n",
    "                              \\int\\limits_{0}^\\infty\\!\\mathrm{d}\\beta \\,\n",
    "                              \\pi_i(\\alpha,\\beta)\n",
    "                              \\ln\\left(\\frac{\\pi_i(\\alpha,\\beta)}{\\phi(\\alpha,\\beta)}\\right)     \n",
    "                      \\right\\}\n",
    "$$\n",
    "is minimized using the SciPy SHGO global optimizer with Nelder-Mead local optimizer to yield the\n",
    "parameter estimates $\\hat{p}$, $\\hat{s}$, $\\hat{n}$, $\\hat{\\nu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp = GammaConjugatePrior.minimum_surprise_estimate(distributions, verbose=True)\n",
    "gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the results\n",
    "\n",
    "Plot the cost function surrounding the minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_i = [GammaConjugatePrior(1, 0, 0, 0).updated(dist) for dist in distributions]\n",
    "\n",
    "def cost_function(p, s, n, v):\n",
    "    try:\n",
    "        gcp = GammaConjugatePrior(p, s, n, v)\n",
    "        return max(gcp.kullback_leibler(gcpi) for gcpi in GCP_i)\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.linspace(np.exp(gcp.lp)-0.01, np.exp(gcp.lp)+0.01, M+1)\n",
    "S = np.linspace(gcp.s-0.05, gcp.s+0.05, M)\n",
    "C = np.array([[cost_function(p, s, gcp.n, gcp.v) for p in P] for s in S])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.linspace(gcp.n-0.01, gcp.n+0.01, M+1)\n",
    "V = np.linspace(gcp.v-0.01, gcp.v+0.01, M)\n",
    "C2 = np.array([[cost_function(np.exp(gcp.lp), gcp.s, n, v)\n",
    "               if (n > v and n-v < 0.008) else np.NaN for n in N] for v in V])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.rc_context({\"xtick.labelsize\" : \"small\", \"ytick.labelsize\" : \"small\"}):\n",
    "    fig = plt.figure(figsize=(6.975, 4.0))\n",
    "    # ax_bg = fig.add_axes((0,0,1,1))\n",
    "    cax = fig.add_axes((0.33, 0.11, 0.34, 0.02))\n",
    "    ax = fig.add_axes((0.09, 0.28, 0.38, 0.71))\n",
    "    Cmax = 10.0\n",
    "    mask = C < Cmax\n",
    "    Cp = C.copy()\n",
    "    Cp[~mask] = np.NaN\n",
    "    mask = C2 < Cmax\n",
    "    Cp2 = C2.copy()\n",
    "    Cp2[~mask] = np.NaN\n",
    "    norm = Normalize(min(Cp[mask].min(),Cp2[mask].min()), Cmax)\n",
    "    h = ax.pcolormesh(P, S, Cp, norm=norm, cmap=batlow, rasterized=True)\n",
    "    ax.scatter(np.exp(gcp.lp), gcp.s, marker='h', facecolor='w', edgecolor='k', linewidth=0.8,\n",
    "               label='Optimum')\n",
    "    ax.set_xlabel('$p$')\n",
    "    ax.set_ylabel('$s$');\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    ax.text(2.5126, 15.42, '(a)', ha='center', va='center')\n",
    "        \n",
    "    ax = fig.add_axes((0.59, 0.28, 0.38, 0.71))\n",
    "    h = ax.pcolormesh(N, V, Cp2, norm=norm, cmap=batlow, rasterized=True)\n",
    "    ax.scatter(gcp.n, gcp.v, marker='h', facecolor='w', edgecolor='k', linewidth=0.8,\n",
    "               label='Optimum')\n",
    "    ax.legend()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.set_xlim((min(xlim[0],ylim[0]), max(xlim[1],ylim[1])))\n",
    "    ax.set_ylim((min(xlim[0],ylim[0]), max(xlim[1],ylim[1])))\n",
    "    ax.add_patch(MPolygon([(xlim[0],ylim[0]), (xlim[1],ylim[1]), (xlim[0], ylim[1])], color='lightgray',zorder=0))\n",
    "    fig.colorbar(h, cax=cax, orientation='horizontal', extend='max')\n",
    "    cax.set_xlabel('Cost function')\n",
    "    ax.set_xlabel('$n$')\n",
    "    ax.set_ylabel('$\\\\nu$');\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_rotation(45);\n",
    "    ax.text(0.2092, 0.2278, '(b)', ha='center', va='center')\n",
    "    \n",
    "    fig.savefig('figures/03-Optimization-Result-psnv.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ctmp = C.copy()\n",
    "Ctmp[np.isnan(Ctmp)] = np.inf\n",
    "print(\"final:      \",cost_function(np.exp(gcp.lp), gcp.s, gcp.n, gcp.v))\n",
    "print(\"raster best:\",Ctmp.min())\n",
    "imin, jmin = np.unravel_index(np.argmin(Ctmp), Ctmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ctmp = C2.copy()\n",
    "Ctmp[np.isnan(Ctmp)] = np.inf\n",
    "print(\"final:      \",cost_function(np.exp(gcp.lp), gcp.s, gcp.n, gcp.v))\n",
    "print(\"raster best:\",Ctmp.min())\n",
    "kmin, lmin = np.unravel_index(np.argmin(Ctmp), Ctmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_logL(a, b, X):\n",
    "    N = X.size\n",
    "    return N * (a * np.log(b) - loggamma(a))  + (a-1) * np.log(X).sum() - b * X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_ANGLE = np.deg2rad(45.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation matrices:\n",
    "rot =  np.array([\n",
    "    [np.cos(XY_ANGLE), np.sin(XY_ANGLE)],\n",
    "    [-np.sin(XY_ANGLE), np.cos(XY_ANGLE)]\n",
    "])\n",
    "roti =  np.array([\n",
    "    [np.cos(XY_ANGLE), -np.sin(XY_ANGLE)],\n",
    "    [np.sin(XY_ANGLE), np.cos(XY_ANGLE)]\n",
    "])\n",
    "\n",
    "def xy2ab(x,y):\n",
    "    return 10.0 ** (rot @ np.stack((x,y)))\n",
    "\n",
    "def ab2xy(a,b):\n",
    "    return roti @ np.log10(np.stack((a,b)))\n",
    "\n",
    "\n",
    "def rotated_plot(ax, gcp, distributions, aticks, bticks, amin=1.0, xmargin=0.3, ypos0=0.1, ypos1=0.99,\n",
    "                 means=[], stds=[], mean_ticks=[], vmin=None, vmax=None):\n",
    "    # Determine maximum likelihood estimates of the distributions:\n",
    "    ai, bi = np.array([gamma_mle(dist, amin=amin)\n",
    "                       for dist in distributions]).T\n",
    "    amin = ai.min()*0.8\n",
    "    amax = ai.max()/0.8\n",
    "    bmin = bi.min() * 0.8\n",
    "    bmax = bi.max() / 0.8\n",
    "    \n",
    "    xi,yi = ab2xy(ai, bi)\n",
    "    xmin = xi.min() - 0.2 * (xi.max() - xi.min())\n",
    "    xmax = xi.max() + 0.2 * (xi.max() - xi.min())\n",
    "    ymin = yi.min() - 0.1 * (yi.max() - yi.min())\n",
    "    ymax = yi.max() + 0.1 * (yi.max() - yi.min())\n",
    "    \n",
    "    xplot2 = np.linspace(xmin, xmax, 300)\n",
    "    yplot2 = np.linspace(ymin, ymax, 700)\n",
    "\n",
    "    xg2,yg2 = np.meshgrid(xplot2, yplot2)\n",
    "    \n",
    "    aplot2, bplot2 = xy2ab(xg2.flatten(), yg2.flatten())\n",
    "    \n",
    "    zp2 = gcp.log_probability(aplot2.flatten(), bplot2.flatten()).reshape(xg2.shape)\n",
    "    zp2 *= np.log10(np.exp(1))\n",
    "    ax.pcolormesh(xg2, yg2, zp2, vmax=vmax, vmin=vmin, cmap=kindlmann_r,\n",
    "                  rasterized=True)\n",
    "    \n",
    "    # For each distribution, plot the uncertainty:\n",
    "    ZG = [gamma_logL(aplot2.flatten(), bplot2.flatten(), dist).reshape(xg2.shape) for dist in distributions]\n",
    "    Z0 = [gamma_logL(a, b, dist) for a,b,dist in zip(ai, bi, distributions)]\n",
    "    uncertainty = [np.count_nonzero(zg > z0-1) for zg,z0 in zip(ZG, Z0)]\n",
    "    \n",
    "    for _, zg, z0 in sorted(zip(uncertainty, ZG, Z0), key=lambda x : -x[0]):\n",
    "        ax.contourf(xplot2, yplot2, zg, levels=[z0 - 1, z0], colors='w', zorder=2, alpha=0.2)\n",
    "    \n",
    "    ax.scatter(*ab2xy(ai,bi), marker='.', edgecolor='none', facecolor='tab:orange', zorder=3)\n",
    "    \n",
    "    ax.set_xlim(xmin-xmargin*(xmax-xmin), xmax+xmargin*(xmax-xmin))\n",
    "    Dy = ymax-ymin\n",
    "    ax.set_ylim(ymin - Dy * ypos0 / (ypos1 - ypos0), ymax + Dy * (1.0 - ypos1) / (ypos1 - ypos0))\n",
    "    ax.add_patch(Rectangle((xmin,ymin), (xmax-xmin), (ymax-ymin), facecolor='none', edgecolor='k',\n",
    "                           linewidth=0.8, zorder=10))\n",
    "    \n",
    "    # Get the axes aspect:\n",
    "    ax.figure.draw_without_rendering()\n",
    "    Dx = xmax-xmin\n",
    "    Dxw = ax.get_window_extent().x1 - ax.get_window_extent().x0\n",
    "    Dyw = ax.get_window_extent().y1 - ax.get_window_extent().y0\n",
    "    \n",
    "    # The oblique alpha ticks right\n",
    "    coord_grid = []\n",
    "    for at in aticks:\n",
    "        coord_grid.append(np.array(ab2xy(np.full(50, at), np.geomspace(1e-2, 1e2, 50))).T)\n",
    "        def plot_atick(a, L, lw=0.8, text=True):\n",
    "            yt = root_scalar(lambda y : np.log(xy2ab(xmax, y)[0]) - np.log(a), x0 = ymin, x1=ymax).root\n",
    "            direction = np.array(ab2xy(a, xy2ab(xmax, yt)[1] / 2)) - np.array((xmax, yt))\n",
    "            direction *= L/np.linalg.norm(direction)\n",
    "            ax.plot((xmax, xmax+direction[0]), (yt, yt+direction[1]), color='k', linewidth=0.8, zorder=0)\n",
    "            if text:\n",
    "                textrot_dx = direction[0] * Dxw / Dx\n",
    "                textrot_dy = direction[1] * Dyw / Dy\n",
    "                text = '$10^{' + str(round(np.log10(a))) + '}$'\n",
    "                ax.text(xmax+1.*direction[0], yt+1.*direction[1]+0.04, text,\n",
    "                        rotation=np.rad2deg(np.arctan2(textrot_dy, textrot_dx)),\n",
    "                        ha='left', va='top', fontsize=8)\n",
    "        plot_atick(at, 0.04)\n",
    "        for asub in 0.1*at*np.arange(2,10):\n",
    "            plot_atick(asub, 0.02, 0.5, False)\n",
    "    \n",
    "    # The oblique beta ticks left:\n",
    "    for bt in bticks:\n",
    "        coord_grid.append(np.array(ab2xy(np.geomspace(1e-1, 1e4, 50), np.full(50, bt))).T)\n",
    "        def plot_btick(b, L, lw=0.8, text=True):\n",
    "            if b < xy2ab(xmin,ymin)[1]:\n",
    "                return\n",
    "            if b > xy2ab(xmin,ymax)[1]:\n",
    "                return\n",
    "            yt = root_scalar(lambda y : np.log(xy2ab(xmin, y)[1]) - np.log(b), x0 = ymin, x1=ymax).root\n",
    "            direction = np.array(ab2xy(xy2ab(xmin, yt)[0] / 2, b)) - np.array((xmin, yt))\n",
    "            direction *= L/np.linalg.norm(direction)\n",
    "            ax.plot((xmin, xmin+direction[0]), (yt, yt+direction[1]), color='k', linewidth=0.8, zorder=0)\n",
    "            if text:\n",
    "                textrot_dx = direction[0] * Dxw / Dx\n",
    "                textrot_dy = direction[1] * Dyw / Dy\n",
    "                text = '$10^{' + str(round(np.log10(b))) + '}$'\n",
    "                ax.text(xmin+3*direction[0], yt+3*direction[1], text,\n",
    "                        rotation=180.0+np.rad2deg(np.arctan2(textrot_dy, textrot_dx)),\n",
    "                        ha='center', va='center', fontsize=8)\n",
    "        plot_btick(bt, 0.04)\n",
    "        for bsub in 0.1*bt*np.arange(2,10):\n",
    "            plot_btick(bsub, 0.02, 0.5, False)\n",
    "\n",
    "    srect = SPoly([(xmin,ymin),(xmax,ymin), (xmax,ymax), (xmin,ymax), (xmin,ymin)])\n",
    "    smls = SMLS(coord_grid)\n",
    "    smls = smls.intersection(srect)\n",
    "    ax.add_collection(LineCollection([g.coords for g in smls.geoms], zorder=1, linewidth=0.5,\n",
    "                                     color='k', linestyle='--'))\n",
    "\n",
    "    # Vertical mean lines:\n",
    "    for mean in means:\n",
    "        aplot = np.array((0.1, 1e4))\n",
    "        line = SLS(np.array(ab2xy(aplot, aplot/mean[0])).T)\n",
    "        line = line.intersection(srect)\n",
    "        ax.plot(*np.array(line.coords).T, color=mean[1], linewidth=0.7)\n",
    "        ax.text(line.coords[0][0], ymax-0.02+mean[2], '$' + str(mean[0]) + '\\,\\\\mathrm{mW}\\,\\\\mathrm{m}^{-2}$',\n",
    "                color='#444444', rotation=90, ha='right', va='top')\n",
    "\n",
    "    # X-Axis:\n",
    "    xticks = []\n",
    "    def mean(a,b):\n",
    "        return a / b\n",
    "    for tick in [10,100, 1000]:\n",
    "        for subtick in range(1,10):\n",
    "            if mean(*xy2ab(xmin,ymin)) > tick*subtick:\n",
    "                continue\n",
    "            if mean(*xy2ab(xmax,ymin)) < tick*subtick:\n",
    "                continue\n",
    "            xt = root_scalar(lambda x : mean(*xy2ab(x,ymin)) - tick*subtick, x0 = xmin, x1=xmax).root\n",
    "            if subtick == 1:\n",
    "                xticks.append([(xt, ymin), (xt, ymin-0.13)])\n",
    "                ax.text(xt, ymin-0.17, str(tick), ha='center', va='top', fontsize=8)\n",
    "            else:\n",
    "                xticks.append([(xt, ymin), (xt, ymin-0.07)])\n",
    "    ax.add_collection(LineCollection(xticks, color='k', linewidth=0.8, zorder=0))\n",
    "    \n",
    "    # Axes labels:\n",
    "    ax.text(0.5*(xmin+xmax), ymin-0.32, 'Average heat flow ($\\\\mathrm{mW}\\,\\\\mathrm{m}^{-2}$)',\n",
    "            va='top', ha='center')\n",
    "    ax.text(ax.get_xlim()[1], 0.5*(ymin+ymax), '$\\\\alpha$', va='center', ha='right', rotation=90)\n",
    "    ax.text(ax.get_xlim()[0], 0.5*(ymin+ymax), '$\\\\beta$ ($\\\\mathrm{mW}^{-1}\\,\\\\mathrm{m}^{2}$)',\n",
    "            va='center', ha='left', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.975,4.1))\n",
    "ax = fig.add_axes((0.075, 0.1, 0.5, 0.89))\n",
    "cax = fig.add_axes((0.58, 0.1, 0.02, 0.89))\n",
    "gcp.visualize(ax, distributions, cmap=kindlmann_r,\n",
    "              q_plot = [(25., 1.0, 50.0, 'lightgray'), (130., 10.0, 1000, 'lightgray')],\n",
    "              qstd_plot = [2.2, (60., 1.1, 50, 'w')], cax=cax, n_alpha=1001, n_beta=1000)\n",
    "ax.legend((Line2D([], [], linestyle='-', linewidth=0.7, color='k'),\n",
    "           Line2D([], [], linestyle=':', linewidth=0.7, color='k'),\n",
    "           Line2D([], [], linestyle='none', marker='.', color='tab:orange')),\n",
    "          ('Distribution average','Distribution standard\\ndeviation',\n",
    "           'Disk covering MLEs'),\n",
    "          loc='lower right')\n",
    "fig.draw_without_rendering()\n",
    "ax.text(np.exp((np.array([0.98, 0.02]) * np.log(ax.get_xlim())).sum()),\n",
    "        np.exp((np.array([0.05, 0.95]) * np.log(ax.get_ylim())).sum()),\n",
    "        '(a)', fontsize=10)\n",
    "#ax_bg = fig.add_axes((0,0,1,1), zorder=-1)\n",
    "# Set limits of the colorbar:\n",
    "cax = fig.axes[1]\n",
    "qm = cax.get_children()[2]\n",
    "cax.set_ylim(qm.norm.vmin, qm.norm.vmax)\n",
    "cax.set_yticks([t for t in range(-10,0)])\n",
    "cax.set_yticklabels([(\"%1.0e\" % 10**t) for t in range(-10,0)])\n",
    "cax.set_ylabel('Prior density ($\\\\mathrm{mW}\\,\\\\mathrm{m}^{-2}$)')\n",
    "\n",
    "\n",
    "# Right panel:\n",
    "ax2 = fig.add_axes((0.69, 0.0, 0.305, 1.0))\n",
    "ax2.set_axis_off()\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "rotated_plot(ax2, gcp, distributions, [10, 100, 1000], [0.1, 1, 10],\n",
    "             means=[(25, 'lightgray', 0), (130, 'lightgray', -0.5)], ypos0=0.1, ypos1=0.99, xmargin=0.28,\n",
    "             vmin=qm.norm.vmin, vmax=qm.norm.vmax)\n",
    "ax2.text((np.array([0.95, 0.05]) * np.array(ax2.get_xlim())).sum(),\n",
    "         (np.array([0.05, 0.95]) * np.array(ax2.get_ylim())).sum(),\n",
    "         '(b)', fontsize=10)\n",
    "ax2.annotate('$\\\\frac{1}{e}\\\\mathcal{L}_\\\\mathrm{max}$',\n",
    "             ab2xy(1.4, 0.04), xytext=ab2xy(0.68, 0.025), zorder=20, color='w',\n",
    "             arrowprops={\n",
    "                 'arrowstyle' : '-|>',\n",
    "                 'color' : 'w',\n",
    "                 'shrinkA' : 0.0,\n",
    "                 'shrinkB' : 0.0\n",
    "             })\n",
    "\n",
    "\n",
    "fig.savefig('figures/03-Gamma-Conjugate-Prior.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QMAX = 250\n",
    "q = np.linspace(0, QMAX, 500)\n",
    "pdf = gcp.posterior_predictive(q)\n",
    "cdf = gcp.posterior_predictive_cdf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tail >= 250 mWm²:\",100 * (1.0 - cdf[-1]),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(0, 1, 200)\n",
    "empirical_cdf_exceedance = np.zeros((z.size, q.size, len(distributions)))\n",
    "cdf_discrete = np.zeros_like(q)\n",
    "for i,dist in enumerate(distributions):\n",
    "    cdf_discrete[:] = 0.0\n",
    "    for qi in dist:\n",
    "        cdf_discrete[q >= qi] += 1.0\n",
    "    cdf_discrete /= dist.size\n",
    "    for j in range(q.size):\n",
    "        empirical_cdf_exceedance[z <= cdf_discrete[j], j, i] = 1\n",
    "\n",
    "empirical_cdf_exceedance = empirical_cdf_exceedance.mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.975, 2.6))\n",
    "ax = fig.add_axes((0.08, 0.175, 0.36, 0.80))\n",
    "ax.plot(q, 100*pdf, label='Prior predictive\\nPDF')\n",
    "ax.set_ylim(0, 2.55)\n",
    "ax.set_xlim(0,QMAX)\n",
    "ax.set_xlabel('Heat flow $q$ ($\\mathrm{mW\\,m}^{-2}$)')\n",
    "ax.set_ylabel('PDF ($10^2\\,\\\\mathrm{mW}^{-1}\\,\\\\mathrm{m}^2$)')\n",
    "ax.axvline(68.3, linewidth=0.8, linestyle='--', color='k',\n",
    "           label='$68.3\\,\\mathrm{mW\\,m}^{-2}$')\n",
    "ax.legend(fontsize='small')\n",
    "ax.text(5, 2.365, \"(a)\")\n",
    "ax = fig.add_axes((0.53, 0.175, 0.36, 0.80))\n",
    "cax = fig.add_axes((0.9, 0.3, 0.02, 0.6))\n",
    "h = ax.pcolormesh(q, z, empirical_cdf_exceedance, cmap=lisbon, rasterized=True)\n",
    "ax.plot(q, cdf, color='w', linewidth=3)\n",
    "ax.plot(q, cdf, color='k', label='Prior predictive\\nCDF')\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xlim(0,QMAX)\n",
    "ax.set_xlabel('Heat flow $q$ ($\\mathrm{mW\\,m}^{-2}$)')\n",
    "ax.set_ylabel('CDF $F$')\n",
    "fig.colorbar(h, cax=cax)\n",
    "cax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "cax.set_yticklabels([\"0\",\"25\",\"50\",\"75\",\"100\"], size='small', rotation=90)\n",
    "cax.set_ylabel(\"Fraction of RGRCD empirical\\nCDF smaller than $F$ (%)\", size='small')\n",
    "ax.legend(fontsize='small', loc='lower right');\n",
    "ax.text(5, 0.93, \"(b)\")\n",
    "fig.savefig('figures/03-Prior-Predictive.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results/05-GCP-Parameters.txt', [[np.exp(gcp.lp), gcp.s, gcp.n, gcp.v]],\n",
    "           header = \"p, s, n, v\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "> Lucazeau, F. (2019). Analysis and mapping of an updated terrestrial heat\n",
    ">    flow data set. Geochemistry, Geophysics, Geosystems, 20, 4001– 4024.\n",
    ">    https://doi.org/10.1029/2019GC008389"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "```\n",
    "A notebook to determine the parameters of the gamma conjugate prior\n",
    "from regional aggregate heat flow distributions.\n",
    "\n",
    "This file is part of the REHEATFUNQ model.\n",
    "\n",
    "Author: Malte J. Ziebarth (ziebarth@gfz-potsdam.de)\n",
    "\n",
    "Copyright © 2019-2022 Deutsches GeoForschungsZentrum Potsdam,\n",
    "            2022 Malte J. Ziebarth\n",
    "            \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}