{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodness-of-Fit, its $R$-Dependence, and Gamma Mixtures\n",
    "This notebook evaluates the goodness-of-fit of the gamma distribution to regional aggregate\n",
    "heat flow distributions obtained from random global $R$-disk coverings (RGRDCs). The impact\n",
    "of $R$, the disk radius, is evaluated to clarify whether large regions might be worse captured\n",
    "by the gamma distribution than smaller regions (assuming the existence of \"large scale\"\n",
    "trends in heat flow).\n",
    "\n",
    "Finally, a mixture of two gamma distribution is used as the model for regional aggregate heat\n",
    "flow, corresponding to the case that the gamma distribution can capture well the variability\n",
    "in a \"homogeneous\" heat flow region but that sometimes the $R$-disk includes the boundary of\n",
    "two such regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pyproj import Proj\n",
    "from pickle import Unpickler\n",
    "from cache import cached_call\n",
    "from scipy.special import erf\n",
    "import matplotlib.pyplot as plt\n",
    "from pdtoolbox import normal_pdf\n",
    "from matplotlib.ticker import FixedLocator\n",
    "from loaducerf3 import Polygon, PolygonSelector\n",
    "from matplotlib.patches import Polygon as MPolygon\n",
    "from pdtoolbox import gamma_pdf\n",
    "from pdtoolbox import normal_mvue, normal_logL, normal_cdf, \\\n",
    "                      frechet_mle, frechet_logL, frechet_cdf, \\\n",
    "                      gamma_mle, gamma_logL, gamma_cdf, \\\n",
    "                      nakagami_mle, nakagami_logL, nakagami_cdf, \\\n",
    "                      log_logistic_mle, log_logistic_logL, log_logistic_cdf, \\\n",
    "                      shifted_gompertz_mle, shifted_gompertz_logL, shifted_gompertz_cdf, \\\n",
    "                      weibull_mle, weibull_logL, weibull_cdf, \\\n",
    "                      log_normal_mle, log_normal_logL, log_normal_cdf, \\\n",
    "                      inverse_gamma_mle, inverse_gamma_logL, inverse_gamma_cdf\n",
    "from pdtoolbox.cython.gamma_accel import gamma_ks_ad_batch\n",
    "from pdtoolbox import GammaDistribution, LogLogisticDistribution\n",
    "from pdtoolbox.gof import LillieforsTable, AndersonDarlingTable\n",
    "\n",
    "from reheatfunq.coverings import random_global_R_disk_coverings\n",
    "from reheatfunq.resilience import generate_synthetic_heat_flow_coverings_mix3, \\\n",
    "                                  generate_normal_mixture_errors_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty figures on HiDPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_continental = np.load('intermediate/heat-flow-selection-mW_m2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate/02-Geometry.pickle','rb') as f:\n",
    "    saf_geometry = Unpickler(f).load()\n",
    "\n",
    "proj_saf = Proj(saf_geometry[\"proj_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate/03-Buffered-Poly.pickle','rb') as f:\n",
    "    buffered_poly = Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(hf_continental.shape[1], dtype=bool)\n",
    "hf_xy = np.stack(proj_saf(*hf_continental[1:3,:]), axis=1)\n",
    "\n",
    "for poly in saf_geometry[\"selection_polygons_xy\"]:\n",
    "    select = PolygonSelector(Polygon(*poly[:-1].T))\n",
    "    mask &= ~select.array_mask(hf_xy)\n",
    "hf_independent = (hf_continental.T)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Gamma.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LG, ADG = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Log-Logistic.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LLL, ADLL = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior for Different Radii\n",
    "Here we investigate, using goodness-of-fit tests, how well the regional aggregate heat\n",
    "flow distributions are described by the gamma distributions.\n",
    "\n",
    "We use the default configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPETITIONS = 200\n",
    "SEED = 890128959\n",
    "DMIN_KM = 20\n",
    "MIN_POINTS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we investigate radii $R$ in a range from $60\\,\\mathrm{km}$ to $260\\,\\mathrm{km}$. For each radius, we\n",
    "generate a number of `REPETITIONS` RGRDCs that overlap in their coverage of the NGHF data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = np.random.SeedSequence(SEED)\n",
    "seeds = sequence.spawn(REPETITIONS)\n",
    "\n",
    "dist_db = {}\n",
    "dist_info_db = {}\n",
    "R_set = [60,70,80,90,100,120,140, 160, 190, 220, 260]\n",
    "for r in R_set:\n",
    "    print(\"r =\",r,\"km\")\n",
    "    dist_db[r] = []\n",
    "    dist_info_db[r] = []\n",
    "    for i in range(REPETITIONS):\n",
    "        valid_points, _, distributions, distribution_lola, distribution_indices \\\n",
    "           = cached_call(random_global_R_disk_coverings, r*1e3, MIN_POINTS, hf_independent,\n",
    "                         buffered_poly, saf_geometry[\"proj_str\"], dmin=DMIN_KM*1e3, seed=seeds[i])\n",
    "        dist_db[r].append([list(dist) for dist in distributions])\n",
    "        dist_info_db[r].append((distribution_indices, valid_points, distribution_lola))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now investigate each of these RGRDCs using the Lilliefors (Kolmgorov-Smirnov)\n",
    "and Anderson-Darling goodness-of-fit tests (Stephens, 1980). For each RGRDC,\n",
    "we compute the rate at which the Lilliefors and AD tests reject the gamma hypothesis\n",
    "for the $R$-disks at $\\alpha=5\\%$. The critical table for the gamma distribution\n",
    "has been computed in notebook `A1-Critical-EDF-Statistics.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_analysis(distribution, dist_db_json, R_set, Ni=100,\n",
    "                          LG_json = json.dumps(LG.to_json(), sort_keys=True),\n",
    "                          ADG_json = json.dumps(ADG.to_json(), sort_keys=True),\n",
    "                          LLL_json = json.dumps(LLL.to_json(), sort_keys=True),\n",
    "                          ADLL_json = json.dumps(ADLL.to_json(), sort_keys=True)):\n",
    "    \"\"\"\n",
    "    Analyzes a distribution for different R.\n",
    "    \"\"\"\n",
    "    # Regenerate key-value pairs to dictionaries:\n",
    "    dist_db = {int(key) : val for key,val in json.loads(dist_db_json).items()}\n",
    "    LG   = LillieforsTable.from_json(json.loads(LG_json))\n",
    "    ADG  = AndersonDarlingTable.from_json(json.loads(ADG_json))\n",
    "    LLL  = LillieforsTable.from_json(json.loads(LLL_json))\n",
    "    ADLL = LillieforsTable.from_json(json.loads(ADLL_json))\n",
    "    print(\"dist_db.keys():\",dist_db.keys())\n",
    "    \n",
    "    \n",
    "    if distribution == \"gamma\":\n",
    "        distribution = GammaDistribution\n",
    "    elif distribution == \"log-logistic\":\n",
    "        distribution = LogLogisticDistribution\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "    # Distribution for the data:\n",
    "    print(\"Starting the iterations\")\n",
    "    n_dists = {}\n",
    "    ad = {}\n",
    "    ks = {}\n",
    "    ad_reject = {}\n",
    "    ks_reject = {}\n",
    "    M_SET = set()\n",
    "    for r in R_set:\n",
    "        n_dists[r] = len(dist_db[r])\n",
    "        m_set = [[len(dist) for dist in dists] for dists in dist_db[r]]\n",
    "        ad_r = []\n",
    "        ks_r = []\n",
    "        adrej_r = []\n",
    "        ksrej_r = []\n",
    "        for dists in dist_db[r]:\n",
    "            ad_r.append([])\n",
    "            ks_r.append([])\n",
    "            adrej_r.append([])\n",
    "            ksrej_r.append([])\n",
    "            for dist in dists:\n",
    "                dist = np.array(dist)\n",
    "                if distribution == GammaDistribution:\n",
    "                    adrej_r[-1].append(ADG.test_reject(dist))\n",
    "                    ksrej_r[-1].append(LG.test_reject(dist))\n",
    "                elif distribution == LogLogisticDistribution:\n",
    "                    adrej_r[-1].append(ADLL.test_reject(dist))\n",
    "                    ksrej_r[-1].append(LLL.test_reject(dist))\n",
    "                ad_r[-1].append(distribution.anderson_darling_statistic(dist))\n",
    "                ks_r[-1].append(distribution.kolmogorov_smirnov_statistic(dist))\n",
    "        M_SET |= set(list(np.concatenate(m_set)))\n",
    "        ad[r] = (ad_r, m_set)\n",
    "        ks[r] = (ks_r, m_set)\n",
    "        ad_reject[r] = adrej_r\n",
    "        ks_reject[r] = ksrej_r\n",
    "    \n",
    "    # Random number generator for reference distribution:\n",
    "    rng = np.random.default_rng(18298)\n",
    "    if distribution == LogLogisticDistribution:\n",
    "        rng_dist = fisk(10)\n",
    "        get_random = lambda m : rng_dist.rvs(m, random_state=rng)\n",
    "    elif distribution == GammaDistribution:\n",
    "        get_random = lambda m : rng.gamma(3.0, size=m)\n",
    "        \n",
    "    \n",
    "    # Reference distribution:\n",
    "    print(\"compute reference distributions.\")\n",
    "    ad_reference =  {}\n",
    "    ks_reference =  {}\n",
    "    if distribution != GammaDistribution:\n",
    "        for m in M_SET:\n",
    "            ad_m = []\n",
    "            ks_m = []\n",
    "            for i in range(Ni):\n",
    "                X = get_random(m)\n",
    "                ad_m.append(distribution.anderson_darling_statistic(X))\n",
    "                ks_m.append(distribution.kolmogorov_smirnov_statistic(X))\n",
    "            ad_reference[m] = np.array(ad_m)\n",
    "            ks_reference[m] = np.array(ks_m)\n",
    "    \n",
    "    return ad, ad_reference, ad_reject, ks, ks_reference, ks_reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fast hash computation in caching, convert the `dist_db` into a key-sorted JSON string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_db_str = json.dumps(dist_db, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad1, ad_reference1, ad_reject1, ks1, ks_reference1, ks_reject1 \\\n",
    "    = cached_call(distribution_analysis, \"gamma\", dist_db_str, R_set, Ni=Ni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one would want to do the same for the log-logistic distribution:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ad0, ad_reference0, ad_reject0, ks0, ks_reference0, ks_reject0 \\\n",
    "    = cached_call(distribution_analysis, \"log-logistic\", list(sorted(dist_db.items())), R_set, Ni=Ni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rejection rates\n",
    "Now, compute the rejection rates for each RGRDC. The rejection rate is computed for each $R$ and for each of the `Ni` RGRDCs per $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_select = np.arange(10,201)\n",
    "rej_rate = np.zeros((len(R_set), REPETITIONS, 2))\n",
    "region_count = np.zeros((len(R_set), REPETITIONS))\n",
    "for i,r in enumerate(R_set):\n",
    "    ad,m = ad1[r]\n",
    "    ks,m = ks1[r]\n",
    "    for l in range(REPETITIONS):\n",
    "        region_count[i,l] = len(ks_reject1[r][l])\n",
    "        rej_rate[i,l,0] = np.count_nonzero(ks_reject1[r][l]) / region_count[i,l] \n",
    "        rej_rate[i,l,1] = np.count_nonzero(ad_reject1[r][l]) / region_count[i,l] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic rejection rate\n",
    "Here, we compute a synthetic data base of RGRDCs which corresponds to the empirical\n",
    "one within the bounds we impose to the synthetic data base:\n",
    "1) The synthetic data base is derived from gamma-distributed data.\n",
    "2) The gamma distributions are parameterized as derived from RGRDC disk samples\n",
    "   using maximum likelihood estimators (the gamma distribution \"look like the data\")\n",
    "   and they have the same sample sizes, mimicking the real-world data within the\n",
    "   boundaries imposed by using a gamma distribution.\n",
    "3) After drawing from the gamma distribution, a relative error is added to the synthetic\n",
    "   data that is similar to the relative error observed in the NGHF data set for 'A' quality\n",
    "   data (whereever an error estimate is available).\n",
    "4) We also apply the limits $0 < q < 250\\,\\mathrm{mWm}^{-2}$ to the heat flow data.\n",
    "\n",
    "With this approach, we aim to understand whether the gamma distribution in addition to some\n",
    "measurement error could reproduce the test results from above.\n",
    "\n",
    "First, we rebuild the 'A' quality error distribution using a mixture of three normal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X00 = 0.08\n",
    "X01 = 0.30\n",
    "X02 = 0.34\n",
    "W0 = 0.39\n",
    "S0 = 0.08\n",
    "S1 = 0.08\n",
    "W1 = 0.3\n",
    "S2 = 0.24\n",
    "W2 = 1 - W0 - W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_pdf(x):\n",
    "    SQ2 = np.sqrt(2)\n",
    "    norm =   W0 * 0.5 * (1.0 - erf(-X00/(SQ2*S0))) \\\n",
    "           + W1 * 0.5 * (1.0 - erf(-X01/(SQ2*S1))) \\\n",
    "           + W2 * 0.5 * (1.0 - erf(-X02/(SQ2*S2)))\n",
    "    return  (W0 * normal_pdf(xplot, X00, S0)\n",
    "             + W1*normal_pdf(xplot, X01, S1)\n",
    "             + W2*normal_pdf(xplot, X02, S2)) / norm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "xplot = np.linspace(0, 0.8, 200)\n",
    "ax.plot(xplot, mixture_pdf(xplot))\n",
    "ax.hist(generate_normal_mixture_errors_3(500000, W0, X00, S0, W1, X01, S1, X02, S2, 983928),\n",
    "        density=True, zorder=0, color='lightgray', bins='auto')\n",
    "print(\"This should look like Fig. S4 from the SI (A quality).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and analyze the synthetic heat flow data base using the same analysis\n",
    "approach as for the NGHF data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_heat_flow_database(repetitions, R_set, dist_db_json, rng_seed,\n",
    "                                          hfmax=250, hfmin=0.0, kmin=1.0, w0=W0, x00=X00, s0=S0,\n",
    "                                          w1=W1, x01=X01, s1=S1, x02=X02, s2=S2):\n",
    "    \"\"\"\n",
    "    Generate a full synthetic global heat flow data base.\n",
    "    \"\"\"\n",
    "    dist_db = {int(key) : [[np.array(dist) for dist in dists] for dists in val]\n",
    "               for key,val in json.loads(dist_db_json).items()}\n",
    "    \n",
    "    # Compute the gamma distribution fits:\n",
    "    print(\"compute gamma fits.\")\n",
    "    gamma_kt = {}\n",
    "    gamma_N = {}\n",
    "    for r in R_set:\n",
    "        gamma_kt[r] = [[GammaDistribution._mle(dist) for dist in dists]\n",
    "                       for dists in dist_db[r]]\n",
    "        gamma_N[r] = [[dist.size for dist in dists] for dists in dist_db[r]]\n",
    "    \n",
    "    print(\"generate synthetic database.\")\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    synthetic_db = {}\n",
    "    for i,R in enumerate(R_set):\n",
    "        coverings = []\n",
    "        K = rng.integers(0, len(dist_db[R]), size=repetitions)\n",
    "        gamma_k = [np.array(gamma_kt[R][k])[:,0] for k in K]\n",
    "        gamma_t = [np.array(gamma_kt[R][k])[:,1] for k in K]\n",
    "        gamma_N_i = [np.array(gamma_N[R][k]) for k in K]\n",
    "        synthetic = generate_synthetic_heat_flow_coverings_mix3(gamma_k, gamma_t, gamma_N_i, hfmax,\n",
    "                                                                w0, x00, s0, w1, x01, s1, x02, s2,\n",
    "                                                                rng.integers(2**63), 12)\n",
    "        coverings += synthetic\n",
    "\n",
    "        synthetic_db[R] = coverings\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"perform AD and KS tests.\")\n",
    "    ad_reject = {}\n",
    "    ks_reject = {}\n",
    "    for i,R in enumerate(R_set):\n",
    "        adrej_r = []\n",
    "        ksrej_r = []\n",
    "        for j in range(repetitions):\n",
    "            # Perform tests:\n",
    "            distributions = synthetic_db[R][j]\n",
    "            dist_n = np.array([len(d) for d in distributions])\n",
    "            ks, ad, k = gamma_ks_ad_batch(distributions, kmin)\n",
    "            adrej_r.append(ad >= ADG(dist_n, k))\n",
    "            ksrej_r.append(ks >= LG(dist_n, k))\n",
    "        ad_reject[R] = adrej_r\n",
    "        ks_reject[R] = ksrej_r\n",
    "    \n",
    "    \n",
    "    print(\"computing rejection rates.\")\n",
    "    rej_rate = np.zeros((len(R_set),repetitions,2))\n",
    "    region_count = np.zeros((len(R_set), repetitions))\n",
    "    for i,R in enumerate(R_set):\n",
    "        for j in range(repetitions):\n",
    "            region_count[i,j] = len(ks_reject[R][j])\n",
    "            rej_rate[i,j,0] = np.count_nonzero(ks_reject[R][j]) / region_count[i,j] \n",
    "            rej_rate[i,j,1] = np.count_nonzero(ad_reject[R][j]) / region_count[i,j]\n",
    "    \n",
    "    return synthetic_db, rej_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, synthetic_rej_rate = \\\n",
    "   generate_synthetic_heat_flow_database(1000, R_set, dist_db_str, 209398, hfmin=0.0, hfmax=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of the NGHF and synthetic data rejection rates for different $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.rc_context({'font.size': 8, 'axes.labelpad': 0.05, 'xtick.major.pad': 1.2, 'ytick.major.pad': 1.2}):\n",
    "    fig = plt.figure(figsize=(6.975, 3.0), dpi=300)\n",
    "    ax = fig.add_axes((0.065, 0.16, 0.36, 0.75))\n",
    "    y = []\n",
    "    c = []\n",
    "    res = []\n",
    "    R_plot = np.array(R_set)\n",
    "    mask = np.ones(len(R_set), dtype=bool)\n",
    "    skip = [70,90]\n",
    "    for i,r in enumerate(R_set):\n",
    "        if r in skip:\n",
    "            mask[i] = False\n",
    "            continue\n",
    "        yi = []\n",
    "        ni = 0\n",
    "        ri = []\n",
    "        for j in range(REPETITIONS):\n",
    "            nj = region_count[i,j]\n",
    "            if nj > 0:\n",
    "                yi.append(rej_rate[i,j,0])\n",
    "                ni += nj\n",
    "                ri.append([1.0 / nj])\n",
    "        y.append(yi)\n",
    "        c.append(ni)\n",
    "        res.append(np.mean(ri) if len(ri) > 0 else 1.0)\n",
    "    R_plot = R_plot[mask]\n",
    "    h0 = ax.boxplot(y, positions=np.array(R_plot)+2, widths=2.5, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='w', linewidth=0.8),\n",
    "                    whiskerprops=dict(linewidth=0.8),\n",
    "                    flierprops=dict(markersize=2, markeredgewidth=0.8))\n",
    "    h1 = ax.boxplot(list(synthetic_rej_rate[mask,:,0]),\n",
    "                    positions=np.array(R_plot)-2, widths=2.5, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='w', linewidth=0.8, edgecolor='gray'),\n",
    "                    whiskerprops=dict(linewidth=0.8, color='gray'),\n",
    "                    flierprops=dict(markersize=1, markeredgewidth=0.8, markeredgecolor='gray'),\n",
    "                    capprops=dict(color='gray'), medianprops=dict(color='tab:blue'))\n",
    "    ax.axhline(0.05, color='k', linewidth=0.5)\n",
    "    ax.set_xlabel('Disk radius $R$ (km)')\n",
    "    ax.set_ylim(0,ax.get_ylim()[1])\n",
    "    # The best selection:\n",
    "    axtw = ax.twinx()\n",
    "    h2 = axtw.plot(R_plot, 100*np.array(res), marker='s', markeredgecolor='none',linestyle=':', color='gray',\n",
    "                   linewidth=0.8, markersize=2)\n",
    "    ax.set_zorder(1)\n",
    "    ax.patch.set_visible(False)\n",
    "    axtw.set_ylabel('$\\langle 100 / \\\\mathrm{\\# disks}\\\\rangle$', fontsize=8, labelpad=2)\n",
    "    axtw.tick_params(axis='y', which='major', pad=1.5)\n",
    "    ax.set_yticks((0.0, 0.05, 0.2, 0.4, 0.6),\n",
    "                  labels=(\"0\", \"5\", \"20\", \"40\", \"60\"),\n",
    "                  fontsize=8)\n",
    "    xticks = [60,80,100,120,140,160,190,220,260]\n",
    "    ax.set_xticks(xticks, labels=(str(x) for x in xticks), fontsize=6)\n",
    "    ax.tick_params(axis='y', which='major', pad=0.5)\n",
    "    ax.set_ylabel('Average rejection rate (%)', fontsize=8, labelpad=2)\n",
    "    ax.set_title('Kolmogorov-Smirnov test', fontsize=10)\n",
    "    axtw.set_ylim(0,100*ax.get_ylim()[1])\n",
    "    ymin,ymax = axtw.get_ylim()\n",
    "    axtw.add_patch(MPolygon([(76,ymin), (104,ymin), (104,ymax), (76,ymax)],\n",
    "                            color='antiquewhite', zorder=0))\n",
    "\n",
    "    ax.legend(handles=((h0[\"boxes\"][0], h0[\"medians\"][0]),\n",
    "                       (h1[\"boxes\"][0], h1[\"medians\"][0]),\n",
    "                       h2[0]),\n",
    "              labels=('NGHF RGRDCs',\n",
    "                      \"Synthetic Gamma\\nwith 'A' quality error\",\n",
    "                      \"Inverse average number\\nof disks in RDRDC\"),\n",
    "              fontsize='small')\n",
    "\n",
    "    #\n",
    "    # Axis 2\n",
    "    #\n",
    "    ax = fig.add_axes((0.575, 0.16, 0.36, 0.75))\n",
    "    y = []\n",
    "    c = []\n",
    "    for i,r in enumerate(R_set):\n",
    "        if r in skip:\n",
    "            continue\n",
    "        yi = []\n",
    "        ni = 0\n",
    "        for j in range(REPETITIONS):\n",
    "            nj = region_count[i,j]\n",
    "            if nj > 0:\n",
    "                yi.append(rej_rate[i,j,1])\n",
    "                ni += nj\n",
    "                ri.append([1.0 / nj])\n",
    "        y.append(yi)\n",
    "        c.append(ni)\n",
    "    h0 = ax.boxplot(y, positions=np.array(R_plot)+2, widths=2.5, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='w', linewidth=0.8),\n",
    "                    whiskerprops=dict(linewidth=0.8),\n",
    "                    flierprops=dict(markersize=2, markeredgewidth=0.8))\n",
    "    h1 = ax.boxplot(list(synthetic_rej_rate[mask,:,1]),\n",
    "                    positions=np.array(R_plot)-2, widths=2.5, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='w', linewidth=0.8, edgecolor='gray'),\n",
    "                    whiskerprops=dict(linewidth=0.8, color='gray'),\n",
    "                    flierprops=dict(markersize=1, markeredgewidth=0.8, markeredgecolor='gray'),\n",
    "                    capprops=dict(color='gray'), medianprops=dict(color='tab:blue'))\n",
    "    ax.axhline(0.05, color='k', linewidth=0.5)\n",
    "    ax.set_xlabel('Disk radius $R$ (km)')\n",
    "    ax.set_ylim(0,ax.get_ylim()[1])\n",
    "    axtw = ax.twinx()\n",
    "    ax.set_zorder(1)\n",
    "    ax.patch.set_visible(False)\n",
    "    h2 = axtw.plot(R_plot, 100 / (np.array(c) / REPETITIONS), marker='x',\n",
    "                   linestyle=':', color='tab:red', linewidth=0.8, markersize=2)\n",
    "    # The best selection:\n",
    "    axtw.set_ylim(0, 100*ax.get_ylim()[1])\n",
    "    ymin,ymax = axtw.get_ylim()\n",
    "    axtw.add_patch(MPolygon([(76,ymin), (104,ymin), (104,ymax), (76,ymax)],\n",
    "                            color='antiquewhite', zorder=0))\n",
    "    axtw.set_ylabel('$\\langle 100 / \\\\mathrm{\\# disks}\\\\rangle$', fontsize=8, labelpad=2)\n",
    "    axtw.tick_params(axis='y', which='major', pad=1.5)\n",
    "    ax.set_yticks((0.0, 0.05, 0.2, 0.4, 0.6),\n",
    "                  labels=(\"0\", \"5\", \"20\", \"40\", \"60\"),\n",
    "                  fontsize=8)\n",
    "    xticks = [60,80,100,120,140,160,190,220,260]\n",
    "    ax.set_xticks(xticks, labels=(str(x) for x in xticks), fontsize=6)\n",
    "    ax.tick_params(axis='y', which='major', pad=0.5)\n",
    "    ax.set_ylabel('Average rejection rate (%)', fontsize=8, labelpad=2)\n",
    "    ax.set_title('Anderson-Darling test', fontsize=10);\n",
    "\n",
    "    ax.legend(handles=((h0[\"boxes\"][0], h0[\"medians\"][0]),\n",
    "                       (h1[\"boxes\"][0], h1[\"medians\"][0]),\n",
    "                       h2[0]),\n",
    "              labels=('NGHF RGRDCs',\n",
    "                      \"Synthetic Gamma\\nwith 'A' quality error\",\n",
    "                      \"Inverse average number\\nof disks in RDRDC\"),\n",
    "              fontsize='small')\n",
    "\n",
    "    fig.savefig('figures/A2-Rejection-Rates-KS-AD-by-R-real-vs-synthetic.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Rejection Rate for Two Gamma Mixture\n",
    "Now we investigate what happens if heat flow were gamma distributed but within a region,\n",
    "mixtures of two different regimes (i.e. gamma distributions) might occur.\n",
    "\n",
    "The size distribution as a function of $R$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_distribution = {}\n",
    "for r in R_set:\n",
    "    sizer = [len(dist) for dists in dist_db[r] for dist in dists]\n",
    "    size_distribution[r] = sizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameterization of the gamma mixture distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_a(x):\n",
    "    return gamma_pdf(x, 50, 1.0)\n",
    "\n",
    "mixture_prob = 0.2\n",
    "mixture_x0   = 25.0\n",
    "mixture_x1   = 50.0\n",
    "mixture_s0   = 5.0\n",
    "mixture_s1   = 7.0\n",
    "mixture_k0 = 25.0\n",
    "mixture_t0 = 1.0\n",
    "mixture_k1 = 50.0\n",
    "mixture_t1 = 1.0\n",
    "\n",
    "mixture2_prob = 0.4\n",
    "mixture2_k0 = 120.0\n",
    "mixture2_t0 = 0.57\n",
    "mixture2_k1 = 50\n",
    "mixture2_t1 = 1.0\n",
    "\n",
    "def density_b(x):\n",
    "    return mixture_prob*gamma_pdf(x, mixture_k0, mixture_t0) \\\n",
    "            + (1.0 - mixture_prob) * gamma_pdf(x, mixture_k1, mixture_t1)\n",
    "\n",
    "def density_c(x):\n",
    "    return mixture2_prob*gamma_pdf(x, mixture2_k0, mixture2_t0) \\\n",
    "            + (1.0 - mixture2_prob) * gamma_pdf(x, mixture2_k1, mixture2_t1)\n",
    "    \n",
    "\n",
    "def generate_sample_a(size,rng):\n",
    "    return rng.gamma(50.0, size=size)\n",
    "\n",
    "\n",
    "def generate_sample_mixture(size, rng, prob, k0, t0, k1, t1):\n",
    "    mix = rng.random(size) <= prob\n",
    "    n0 = np.count_nonzero(mix)\n",
    "    n1 = size - n0\n",
    "    s = np.empty(size)\n",
    "    s[mix]  = t0 * rng.gamma(k0, size=n0)\n",
    "    s[~mix] = t1 * rng.gamma(k1, size=n1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the previous analyses for synthetic data from these gamma mixture distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mixture_rejection_power(repetitions, R_set, size_distribution_list, seed,\n",
    "                                     mix0_prob, mix0_k0, mix0_t0, mix0_k1, mix0_t1,\n",
    "                                     mix1_prob, mix1_k0, mix1_t0, mix1_k1, mix1_t1,\n",
    "                                     ADG_json_str):\n",
    "    \"\"\"\n",
    "    Compute the power of the Anderson-Darling test against\n",
    "    \"\"\"\n",
    "    ADG = AndersonDarlingTable.from_json(json.loads(ADG_json_str))\n",
    "    perc = 0\n",
    "    rng = np.random.default_rng(seed)\n",
    "    power = {}\n",
    "    for j,r in enumerate(R_set):\n",
    "        powerr = np.zeros(3)\n",
    "        sizes = rng.choice(size_distribution_list[j], size=repetitions)\n",
    "        for i in range(repetitions):\n",
    "            if j * repetitions + i > 0.01 * perc * repetitions * len(R_set):\n",
    "                print(\" \",perc,\"% of samples generated\")\n",
    "                perc += 5\n",
    "            sai = generate_sample_a(sizes[i], rng)\n",
    "            sbi = generate_sample_mixture(sizes[i], rng, mix0_prob, mix0_k0, mix0_t0, mix0_k1, mix0_t1)\n",
    "            sci = generate_sample_mixture(sizes[i], rng, mix1_prob, mix1_k0, mix1_t0, mix1_k1, mix1_t1)\n",
    "            powerr[0] += ADG.test_reject(sai)\n",
    "            try:\n",
    "                powerr[1] += ADG.test_reject(sbi)\n",
    "            except:\n",
    "                print(\"sbi:\", sbi.size)\n",
    "                print(sbi)\n",
    "                print(\"gamma:\", gamma_mle(sbi, kmin=1.0))\n",
    "                raise RuntimeError()\n",
    "            powerr[2] += ADG.test_reject(sci)\n",
    "        powerr /= repetitions\n",
    "        power[r] = powerr\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = 10000\n",
    "power = cached_call(evaluate_mixture_rejection_power, repetitions, R_set,\n",
    "                    [np.array(size_distribution[r]) for r in R_set], 8938,\n",
    "                    mixture_prob, mixture_k0, mixture_t0, mixture_k1, mixture_t1,\n",
    "                    mixture2_prob, mixture2_k0, mixture2_t0, mixture2_k1, mixture2_t1,\n",
    "                    json.dumps(ADG.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.rc_context({'axes.labelpad': 2.5, 'xtick.major.pad': 1.2, 'ytick.major.pad': 1.2,\n",
    "                     'xtick.labelsize' : 6, 'ytick.labelsize' : 6, 'axes.labelsize' : 8}):\n",
    "    fig = plt.figure(figsize=(5.63250, 2.5), dpi=300)\n",
    "    #ax_bg = fig.add_axes((0,0,1,1))\n",
    "\n",
    "    color0 = 'tab:blue'\n",
    "    color1 = 'tab:orange'\n",
    "\n",
    "    ax = fig.add_axes((0.083, 0.13, 0.5, 0.8))\n",
    "    #ax.plot(R_set, [power[r][0] for r in R_set])\n",
    "    h0 = ax.plot(R_set, [power[r][1] for r in R_set], marker='.', color=color0, linewidth=1)\n",
    "    h1 = ax.plot(R_set, [power[r][2] for r in R_set], marker='.', color=color1, linewidth=1)\n",
    "    mix = 0.28\n",
    "    ax.set_ylim(0.0, ax.get_ylim()[1])\n",
    "    h2 = ax.plot(R_set, mix*np.array([power[r][1] for r in R_set]) + (1.0 - mix)*0.05,\n",
    "                 color=color0, linestyle='--', linewidth=0.8)\n",
    "    ax.boxplot(list(rej_rate[:,:,1]),\n",
    "               positions=np.array(R_set), widths=5, patch_artist=True,\n",
    "               boxprops=dict(facecolor='w', linewidth=0.8, edgecolor='k'),\n",
    "               whiskerprops=dict(linewidth=0.8, color='k'),\n",
    "               flierprops=dict(markersize=1, markeredgewidth=0.8, markeredgecolor='k'),\n",
    "               capprops=dict(color='k'), medianprops=dict(color='tab:blue'))\n",
    "\n",
    "    ax.axhline(0.05, color='k', linewidth=0.8)\n",
    "    ax.set_xlabel(\"Disk radius $R$ (km)\")\n",
    "    ax.set_ylabel('Anderson-Darling rejection rate', fontsize='small')\n",
    "    xticks = [60,80,100,120,140,160,190,220,260]\n",
    "    ax.set_xticks(xticks, labels=(str(x) for x in xticks))\n",
    "    ax.set_yticks(ax.get_yticks(), labels=(str(round(x,1)) for x in ax.get_yticks()))\n",
    "    ax.xaxis.set_minor_locator(FixedLocator([70,90]))\n",
    "    ax.patch.set_facecolor('none')\n",
    "    ax.set_facecolor('none')\n",
    "    ax.legend(handles=(h0[0],h1[0],h2[0]),\n",
    "              labels=('Γ mix 0',\n",
    "                      'Γ mix 1',\n",
    "                      f'{int(100*mix)} % Γ mix 0\\n+ {int(100*(1-mix))} % pure Γ'),\n",
    "              fontsize=8,\n",
    "              loc=('center right'),\n",
    "              bbox_to_anchor=(0.5, 0.25, 0.5, 0.75))\n",
    "\n",
    "\n",
    "    xplot = np.linspace(0,110,200)[1:]\n",
    "    ax = fig.add_axes((0.66, 0.13, 0.33, 0.8))\n",
    "    h0 = ax.plot(xplot, 100*density_b(xplot), color=color0)\n",
    "    ax.plot(xplot, 100*mixture_prob * gamma_pdf(xplot, mixture_k0, mixture_t0), linestyle=':', color=color0,\n",
    "            linewidth=0.8)\n",
    "    ax.plot(xplot, 100*(1-mixture_prob) * gamma_pdf(xplot, mixture_k1, mixture_t1), linestyle=':', color=color0,\n",
    "            linewidth=0.8)\n",
    "\n",
    "    h1 = ax.plot(xplot, 100*density_c(xplot), color=color1)\n",
    "    ax.plot(xplot, 100*mixture2_prob * gamma_pdf(xplot, mixture2_k0, mixture2_t0), linestyle=':', color=color1,\n",
    "            linewidth=0.8)\n",
    "    ax.plot(xplot, 100*(1-mixture2_prob) * gamma_pdf(xplot, mixture2_k1, mixture2_t1), linestyle=':', color=color1,\n",
    "            linewidth=0.8)\n",
    "    ax.set_xlabel('Heat flow ($\\mathrm{mW}\\mathrm{m}^{-2}$)');\n",
    "    ax.set_ylabel('Density ($10^{-2}\\,\\mathrm{m^2}\\mathrm{mW}^{-1}$)');\n",
    "    ax.legend(handles=(h0[0],h1[0],Line2D([], [], color='k', linestyle=':', linewidth=0.8)),\n",
    "              labels=('Γ mix 0',\n",
    "                      'Γ mix 1',\n",
    "                      f'Components'),\n",
    "              fontsize=6)\n",
    "    fig.savefig('figures/A2-Gamma-Mix-AD-Rejections.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "> Stephens, M. A. (1980). \"Tests based on EDF statistics\". In: Stephens, M. A. \\& D'Agostino, R. B. *Goodness-of-Fit Technieques*. Marcehl Dekker, Inc.; New York."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "```\n",
    "A notebook to evaluate the goodness-of-fit and its R-dependence\n",
    "of the gamma distribution model for regional aggregate heat flow\n",
    "distributions.\n",
    "\n",
    "This file is part of the REHEATFUNQ model.\n",
    "\n",
    "Author: Malte J. Ziebarth (ziebarth@gfz-potsdam.de)\n",
    "\n",
    "Copyright © 2019-2022 Deutsches GeoForschungsZentrum Potsdam,\n",
    "            2022 Malte J. Ziebarth\n",
    "            \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}