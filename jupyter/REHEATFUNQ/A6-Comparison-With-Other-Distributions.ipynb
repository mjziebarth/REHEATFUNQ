{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison With Other Distributions\n",
    "This notebook investigates the fit of the gamma distribution to the regional\n",
    "aggregate heat flow distributions from the *Random Global $R$-Disk Coverings* (RGRDCs)\n",
    "of the NGHF data set (Lucazeau, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pyproj import Proj\n",
    "from plotconfig import *\n",
    "from cmcrameri.cm import *\n",
    "from pickle import Unpickler\n",
    "from cache import cached_call\n",
    "import matplotlib.pyplot as plt\n",
    "from zeal2022hf import get_cm_colors\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "from loaducerf3 import Polygon, PolygonSelector\n",
    "from pdtoolbox.gof import LillieforsTable, AndersonDarlingTable\n",
    "from pdtoolbox import FrechetDistribution, GammaDistribution, InverseGammaDistribution, \\\n",
    "                      LogLogisticDistribution, LogNormalDistribution, \\\n",
    "                      NakagamiDistribution, NormalDistribution, ShiftedGompertzDistribution, \\\n",
    "                      WeibullDistribution\n",
    "from reheatfunq.coverings import random_global_R_disk_coverings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_continental = np.load('intermediate/heat-flow-selection-mW_m2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate/02-Geometry.pickle','rb') as f:\n",
    "    saf_geometry = Unpickler(f).load()\n",
    "\n",
    "proj_str = saf_geometry[\"proj_str\"]\n",
    "proj_saf = Proj(saf_geometry[\"proj_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate/03-Buffered-Poly.pickle','rb') as f:\n",
    "    buffered_poly = Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(hf_continental.shape[1], dtype=bool)\n",
    "hf_xy = np.stack(proj_saf(*hf_continental[1:3,:]), axis=1)\n",
    "\n",
    "for poly in saf_geometry[\"selection_polygons_xy\"]:\n",
    "    select = PolygonSelector(Polygon(*poly[:-1].T))\n",
    "    mask &= ~select.array_mask(hf_xy)\n",
    "hf_independent = (hf_continental.T)[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Critical Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Frechet.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LF, ADF = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Gamma.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LG, ADG = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Inverse-Gamma.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LIG, ADIG = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Log-Logistic.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LLL, ADLL = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Log-Normal.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LLN, ADLN = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Nakagami.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LNAK, ADNAK = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Normal.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LN, ADN = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Shifted-Gompertz.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LSG, ADSG = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate/A1-Critical-Weibull.json\", 'r') as f:\n",
    "    LA = json.load(f)\n",
    "    LW, ADW = LillieforsTable.from_json(LA[0]), AndersonDarlingTable.from_json(LA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTIONS = [(GammaDistribution, LG, ADG),\n",
    "                 (LogLogisticDistribution, LLL, ADLL),\n",
    "                 (NormalDistribution, LN, ADN),\n",
    "                 (FrechetDistribution, LF, ADF),\n",
    "                 (InverseGammaDistribution, LIG, ADIG),\n",
    "                 (LogNormalDistribution, LLN, ADLN),\n",
    "                 (NakagamiDistribution,LNAK, ADNAK),\n",
    "                 (ShiftedGompertzDistribution, LSG, ADSG),\n",
    "                 (WeibullDistribution, LW, ADW)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 80e3\n",
    "MIN_POINTS = 10\n",
    "DMIN = 20e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic(x, logL, k):\n",
    "    \"\"\"\n",
    "    Computes the Bayesian information criterion.\n",
    "\n",
    "    Arguments:\n",
    "       x     : Sample points\n",
    "       logLs : List of log-likelihoods of the models\n",
    "               to consider. The models have to be\n",
    "               computed using the maximum likelihood\n",
    "               estimator on the data x.\n",
    "       k     : Number of parameter. Has to be a list or\n",
    "               an integer, the latter if all models have\n",
    "               the same number of parameters.\n",
    "    \"\"\"\n",
    "    n = x.size\n",
    "    return k * np.log(n) - 2*lL(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distributions(data_sets, distributions, alpha=0.05, silent_fail=False):\n",
    "    \"\"\"\n",
    "    Performs a probability distribution analysis: Enumerates\n",
    "    the data set and for each perform goodness-of-fit tests\n",
    "    for all the PDF models given (`distributions`).\n",
    "    \n",
    "    Arguments:\n",
    "       data_sets     : List of data sets.\n",
    "       distributions : List of tuples, each characterizing a\n",
    "                       probability distribution. Each tuple should\n",
    "                       have the following components:\n",
    "\n",
    "                       (label, mle, logL, cdf, (test1, ...), nparams)\n",
    "\n",
    "                       where `label` is the name of the distribution,\n",
    "                       `mle` a maximum-likelihood estimator, `logL` the\n",
    "                       distribution's log-likelihood, and `cdf` the\n",
    "                       distribution's cumulative distribution function.\n",
    "                       The latter three should be callable.\n",
    "                       Finally, `(test1, ...)` should be an enumeration of\n",
    "                       critical test table instances, i.e. LillieforsTable\n",
    "                       and AndersonDarlingTable instances, and\n",
    "                       `nparams` an integer quantifying the number of\n",
    "                       parameters.\n",
    "    \"\"\"\n",
    "    # Initialize the goodness-of-fit result arrays:\n",
    "    M = len(distributions)\n",
    "    reject = np.ones((len(data_sets), M, 2), dtype=bool)\n",
    "    bic = np.zeros((len(data_sets), M))\n",
    "    \n",
    "    \n",
    "    # Perform the analysis:\n",
    "    for i,data in enumerate(data_sets):\n",
    "        logLs = []\n",
    "        for j in range(M):\n",
    "            # Shortcut for negative data:\n",
    "            if np.any(data < distributions[j][0].xinf()):\n",
    "                aic[i,j] = np.inf\n",
    "                reject[i,j,:] = True\n",
    "                continue\n",
    "                \n",
    "            # Perform the tests:\n",
    "            for l,test in enumerate(distributions[j][1:3]):\n",
    "                try:\n",
    "                    reject[i,j,l] = test.test_reject(data)\n",
    "                except Exception as e:\n",
    "                    if not silent_fail:\n",
    "                        print(\"data:\",data)\n",
    "                        print(\"distribution:\",distributions[j][0])\n",
    "                        print(\"MLE:\", distributions[j][0].mle(data)._params)\n",
    "                        print(\"data size:\", data.size)\n",
    "                        #raise e\n",
    "                        \n",
    "            # MLE to establish parameters:\n",
    "            bic[i,j] = distributions[j][0].mle(data).bic(data)        \n",
    "    \n",
    "    return reject, bic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will output some hierachical results. The data will be indexable and the indices\n",
    "iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_regional_analysis_backend(N, R, min_points, hf_independent, exclusion_poly,\n",
    "                                          distributions, dmin=DMIN, proj_str=proj_str, alpha=0.05,\n",
    "                                          seed=98764655947253418677864477968794449493):\n",
    "    \"\"\"\n",
    "    Performs a number of global analyses, for each determining a nearly-covering\n",
    "    set of regional heat flow distributions\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "    all_lilliefors = []\n",
    "    all_anderson_darling = []\n",
    "    all_aic = []\n",
    "    all_bic = []\n",
    "    all_distributions = []\n",
    "    gamma_params = []\n",
    "    K = 0\n",
    "    M = len(distributions)\n",
    "    \n",
    "    seeds = np.random.SeedSequence(seed).spawn(N)\n",
    "    for i in range(N):\n",
    "        print(\"i =\",i,\"/\",N)\n",
    "        \n",
    "        # Determine the regional distributions:\n",
    "        central_points, _, hf_distributions, hf_lola, distribution_indices \\\n",
    "           = cached_call(random_global_R_disk_coverings, R, min_points, hf_independent,\n",
    "                         buffered_poly, proj_str, dmin=dmin, seed=seeds[i])\n",
    "        K += len(central_points)\n",
    "        \n",
    "        # For reproducibility, save the distributions:\n",
    "        all_distributions += hf_distributions\n",
    "        \n",
    "        # Obtain gamma fits:\n",
    "        gamma_params.extend(GammaDistribution.mle(dist) for dist in hf_distributions)\n",
    "        \n",
    "        # Analyze the distributions:\n",
    "        reject, bic = analyze_distributions(hf_distributions, distributions, alpha=0.05, silent_fail=False)\n",
    "        \n",
    "        # Collect the data:\n",
    "        all_points.extend(central_points)\n",
    "        all_lilliefors.append(reject[..., 0])\n",
    "        all_anderson_darling.append(reject[..., 1])\n",
    "        all_bic.append(bic)\n",
    "        \n",
    "        \n",
    "    points = np.concatenate(all_points, axis=0)\n",
    "    tests = np.empty((K, M, 3))\n",
    "    tests[:, :, 0] = [l for L in all_lilliefors for l in L]\n",
    "    tests[:, :, 1] = [l for L in all_anderson_darling for l in L]\n",
    "    tests[:, :, 2] = [l for L in all_bic for l in L]\n",
    "    \n",
    "    rejection_rates = np.empty((N, M, 2))\n",
    "    rejection_rates[:, :, 0] = [ll.mean(axis=0) for ll in all_lilliefors]\n",
    "    rejection_rates[:, :, 1] = [ad.mean(axis=0) for ad in all_anderson_darling]\n",
    "    \n",
    "\n",
    "    return points, tests, rejection_rates, gamma_params, all_distributions, all_aic, all_bic\n",
    "\n",
    "\n",
    "# We call this cached function from a convenience wrapper:\n",
    "\n",
    "def monte_carlo_regional_analysis(N, R, min_points, hf_independent, exclusion_poly, distributions, dmin, alpha):\n",
    "    \"\"\"\n",
    "    Performs a number of global analyses, for each determining a nearly-covering\n",
    "    set of regional heat flow distributions\n",
    "    \"\"\"\n",
    "    return cached_call(monte_carlo_regional_analysis_backend, N, R, min_points, hf_independent, exclusion_poly,\n",
    "                       distributions, dmin=dmin, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_points, mc_tests, mc_rejection_rates, mc_gamma_params, mc_all_distributions, mc_all_aic, mc_all_bic\\\n",
    "    = monte_carlo_regional_analysis(100, R, MIN_POINTS, hf_independent, buffered_poly, DISTRIBUTIONS, dmin=DMIN,\n",
    "                                    alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_delta_bic(bic):\n",
    "    bic = bic.copy()\n",
    "    bic[np.isnan(bic)] = np.inf\n",
    "    select = np.argmin(bic)\n",
    "    mask = np.ones(bic.size, dtype=bool)\n",
    "    mask[select] = False\n",
    "    delta_bic = bic[mask].min() - bic[select]\n",
    "    if np.isnan(delta_bic):\n",
    "        print(bic[mask])\n",
    "    return delta_bic\n",
    "\n",
    "bic_all = np.concatenate(mc_all_bic, axis=0)\n",
    "bic_select = [np.argmin(bic, axis=1) for bic in mc_all_bic]\n",
    "delta_bic = [np.array([compute_delta_bic(b) for b in bic]) for bic in mc_all_bic]\n",
    "bic_select_all = np.concatenate(bic_select, dtype=int)\n",
    "delta_bic_all = np.concatenate(delta_bic)\n",
    "bic_critical_select = bic_select_all[delta_bic_all > 2]\n",
    "\n",
    "delta_bic_nonselect = np.array([bic_all[i,bic_select_all[i]] for i in range(bic_all.shape[0])])[:,np.newaxis] \\\n",
    "                      - bic_all\n",
    "delta_bic_nonselect = [delta_bic_nonselect[~np.isnan(delta_bic_nonselect[:,i]), i] for i in range(9)]\n",
    "\n",
    "delta_bic_set = [delta_bic_all[bic_select_all == i] for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_vs_gof = np.zeros((bic_select_all.size,9,2), dtype=bool)\n",
    "for i in range(bic_select_all.size):\n",
    "    bic_vs_gof[i,bic_select_all[i],0] = True\n",
    "bic_vs_gof[:,:,1] = mc_tests[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    GammaDistribution : \"Gamma\",\n",
    "    FrechetDistribution : \"Fréchet\",\n",
    "    InverseGammaDistribution : \"Inverse gamma\",\n",
    "    LogNormalDistribution : \"Log Normal\",\n",
    "    LogLogisticDistribution : \"Log Logistic\",\n",
    "    NakagamiDistribution : \"Nakagami\",\n",
    "    NormalDistribution : \"Normal\",\n",
    "    ShiftedGompertzDistribution : \"Shifted Gompertz\",\n",
    "    WeibullDistribution : \"Weibull\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about which distributions have ΔBIC > 2 when being selected (all relative to the total number of samples investigated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraction of total samples which have dBIC>2:\")\n",
    "for i,dist in enumerate(DISTRIBUTIONS):\n",
    "    print(label_dict[dist[0]],\":\", 100*np.count_nonzero(delta_bic_set[i] >= 2.0) / sum(len(x) for x in mc_all_bic))\n",
    "print(\"total:\",100*sum(np.count_nonzero(delta_bic_set[i] >= 2.0) / sum(len(x) for x in mc_all_bic)\n",
    "                      for i in range(len(DISTRIBUTIONS))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = get_cm_colors(vik, 7)\n",
    "color0 = colors[0]\n",
    "color1 = colors[4]\n",
    "color2 = colors[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.rc_context({'axes.labelpad': 0.1, 'xtick.major.pad': 1.2, 'ytick.major.pad': 1.2}):\n",
    "    fig = plt.figure(figsize=(6.975, 4.0), dpi=200)\n",
    "    # ax_bg = fig.add_axes((0,0,1,1)) # Design canvas\n",
    "\n",
    "    vx = np.arange(9)+0.1\n",
    "    \n",
    "    ax0 = fig.add_axes((0.70, 0.2, 0.28, 0.78))\n",
    "    ax0.text(0.94, -0.4, '(c)', ha='center', va='center')\n",
    "    ax0.barh(np.arange(9)[::-1]+0.15, bic_vs_gof[:,:,0].mean(axis=0), height=0.3, label='BIC selection',\n",
    "             color=color0)\n",
    "    ax0.barh(np.arange(9)[::-1]-0.15, bic_vs_gof[:,:,1].mean(axis=0), height=0.3, label='AD rejection',\n",
    "             color=color1)\n",
    "    ax0.set_yticks(range(9)[::-1])\n",
    "    ax0.set_yticklabels([label_dict[d[0]].replace(' ','\\n') for d in DISTRIBUTIONS]);\n",
    "    ax0.set_xlim(0,1.0)\n",
    "    ax0.set_xticks([0,0.25, 0.5, 0.75, 1.0])\n",
    "    ax0.set_xticklabels([0, 25, 50, 75, 100])\n",
    "    ax0.set_xlabel('Selection rate (%)')\n",
    "    ax0.legend()\n",
    "    \n",
    "    dy_pos = 6\n",
    "    dy_neg = 40\n",
    "    dy_nrm = dy_pos + dy_neg\n",
    "    dy_pos_rel = dy_pos / dy_nrm\n",
    "    dy_neg_rel = dy_neg / dy_nrm\n",
    "    dy_tot = (0.185 + 0.61) # total axes height available in the figure\n",
    "    \n",
    "    #\n",
    "    # The positive dBIC:\n",
    "    #\n",
    "    ax01 = fig.add_axes((0.062, 1.0 - dy_pos_rel * dy_tot - 0.011, 0.53, dy_pos_rel * dy_tot))# ax0.twinx()\n",
    "    ax01.text(-0.3, 4.9, '(a)', ha='center', va='center')\n",
    "    ax01.set_ylim(0, dy_pos)\n",
    "    h0 = ax01.boxplot(delta_bic_set, positions=np.arange(9), patch_artist=True,\n",
    "                      whis=(0, 95),\n",
    "                      boxprops=dict(facecolor='w', linewidth=0.8),\n",
    "                      medianprops=dict(color=color2),\n",
    "                      whiskerprops=dict(linewidth=0.8),\n",
    "                      capprops=dict(linewidth=0.8),\n",
    "                      flierprops=dict(markersize=1.5, markeredgecolor='None',\n",
    "                                      marker='s', markerfacecolor='k'))\n",
    "    ax01.axhline(2.0, color='gray', linewidth=0.8, linestyle=':')\n",
    "    ax01.set_ylabel('$\\\\Delta\\\\mathrm{BIC}$\\nif selected', labelpad=1.3, loc='top')\n",
    "    ax01.set_xticks(range(9))\n",
    "    ax01.set_yticks([0,2,4,6])\n",
    "    ax01.set_xticklabels([label_dict[d[0]].replace(' ','\\n') for d in DISTRIBUTIONS], rotation=90);\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # The negative dBIC:\n",
    "    #\n",
    "    ax1 = fig.add_axes((0.062, 0.02, 0.53, dy_neg_rel * dy_tot))\n",
    "    ax1.text(-0.2, -38.0, '(b)', ha='center', va='center')\n",
    "    h0 = ax1.boxplot(delta_bic_nonselect, positions=np.arange(9), patch_artist=True,\n",
    "                     whis=(5, 100),\n",
    "                     boxprops=dict(facecolor='w', linewidth=0.8),\n",
    "                     medianprops=dict(color=color2),\n",
    "                     whiskerprops=dict(linewidth=0.8),\n",
    "                     capprops=dict(linewidth=0.8),\n",
    "                     flierprops=dict(markersize=1.5, markeredgecolor='None',\n",
    "                                     marker='s', markerfacecolor='k'))\n",
    "    ax1.set_xlim(ax01.get_xlim())\n",
    "    ax1.plot(ax01.get_xlim(), (-6, -6), zorder=0, color='gray', linewidth=0.8, linestyle=':')\n",
    "    ax1.set_ylim(-40,0)\n",
    "    ax1.set_ylabel('$\\\\Delta \\\\mathrm{BIC}$ if not selected', va='bottom')\n",
    "    ax1.xaxis.tick_top()\n",
    "    ax1.set_xticks(range(9))\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticks((-40,-20,-6))\n",
    "\n",
    "    \n",
    "    # Custom Boxplot legend:\n",
    "    ax1.add_patch(FancyBboxPatch((4.5, -39), 3.9, 10, facecolor='none', edgecolor=[0.8]*3,\n",
    "                                 boxstyle=\"round,pad=0,rounding_size=0.2\"))\n",
    "    ax2 = fig.add_axes((0.365, 0.119, 0.2, 0.07))\n",
    "    ax2.set_xlim(0,100)\n",
    "    ax2.set_ylim(0.9, 1.1)\n",
    "    xbp = np.linspace(0,100,41)\n",
    "    ax2.boxplot(xbp, vert=False, patch_artist=True,\n",
    "                whis=(5, 100),\n",
    "                     boxprops=dict(facecolor='w', linewidth=0.8),\n",
    "                     medianprops=dict(color=color2),\n",
    "                     whiskerprops=dict(linewidth=0.8, clip_on=False),\n",
    "                     capprops=dict(linewidth=0.8, clip_on=False),\n",
    "                     flierprops=dict(markersize=1.5, markeredgecolor='None',\n",
    "                                     marker='s', markerfacecolor='k', clip_on=False))\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_xticks([0, 5, 25, 50, 75, 100])\n",
    "    ax2.set_xticklabels([\"0\",\"5\",\"25\",\"50\",\"75\",\"100\"], fontsize='small')\n",
    "    ax2.spines.right.set_visible(False)\n",
    "    ax2.spines.top.set_visible(False)\n",
    "    ax2.spines.left.set_visible(False)\n",
    "    ax2.set_xlabel('$\\Delta$BIC Quantile (%)')\n",
    "    \n",
    "    fig.savefig('figures/A6-Comparison-Various-Distributions-BIC-GOF.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "> Lucazeau, F. (2019). Analysis and mapping of an updated terrestrial heat\n",
    ">    flow data set. Geochemistry, Geophysics, Geosystems, 20, 4001– 4024.\n",
    ">    https://doi.org/10.1029/2019GC008389"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "```\n",
    "A notebook to perform model selection for regional aggregate heat flow\n",
    "among various univariate probability distributions.\n",
    "\n",
    "This file is part of the REHEATFUNQ model.\n",
    "\n",
    "Author: Malte J. Ziebarth (ziebarth@gfz-potsdam.de)\n",
    "\n",
    "Copyright © 2019-2022 Deutsches GeoForschungsZentrum Potsdam,\n",
    "            2022 Malte J. Ziebarth\n",
    "            \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}